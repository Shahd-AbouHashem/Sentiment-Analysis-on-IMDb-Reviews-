# -*- coding: utf-8 -*-
"""Task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12lUE-w9wbyDQwW5nZjU97ef1T8uSGaZa

import libraries
"""

!pip install nltk wordcloud

import pandas as pd
import nltk
import re
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import os
import joblib
import string
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from collections import Counter

# necessary NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

"""Download dataset"""

df = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')
print(df.head())
df.columns
print(df['sentiment'].value_counts())

""" Clean, Preprocess, and Filter Text"""

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Final clean function (no prints)
def clean_text(text):
    text = text.lower()
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    words = text.split()
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2]
    return ' '.join(words)

# Apply to the full dataset
df['clean_review'] = df['review'].apply(clean_text)

# Show the result
print("\n Cleaned Data Sample:")
print(df[['review', 'clean_review', 'sentiment']].head())

"""Convert Text to Numerical Format"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=5000)

# Fit and transform the cleaned review text
X = vectorizer.fit_transform(df['clean_review']).toarray()
y = df['sentiment']

print("\nShape of the number data:", X.shape)

"""Train and Evaluate Logistic Regression"""

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# Train the model
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)

# Predict on test set
y_pred_lr = lr_model.predict(X_test)

# Evaluate
print(f"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr) * 100:.2f}%\n")
print("Classification Report:\n", classification_report(y_test, y_pred_lr))

"""***Bonus*** Visualize Most Frequent Positive/Negative Words


"""

import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Separate positive and negative reviews
positive_reviews = df[df['sentiment'] == 1]['clean_review']
negative_reviews = df[df['sentiment'] == 0]['clean_review']

# Transform them separately using the SAME vectorizer
tfidf_pos = vectorizer.transform(positive_reviews)
tfidf_neg = vectorizer.transform(negative_reviews)

# Get average TF-IDF scores per word
pos_avg = np.asarray(tfidf_pos.mean(axis=0)).ravel()
neg_avg = np.asarray(tfidf_neg.mean(axis=0)).ravel()

# Get feature names (words)
words = np.array(vectorizer.get_feature_names_out())

# Sort words by their average TF-IDF scores
pos_sorted = sorted(zip(words, pos_avg), key=lambda x: x[1], reverse=True)
neg_sorted = sorted(zip(words, neg_avg), key=lambda x: x[1], reverse=True)

# Create dictionaries of word frequencies for word clouds
pos_freq = {word: score for word, score in pos_sorted[:50]}  # top 50 positive words
neg_freq = {word: score for word, score in neg_sorted[:50]}  # top 50 negative words

# Generate word clouds
pos_wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(pos_freq)
neg_wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(neg_freq)

# Plot positive
plt.figure(figsize=(10,5))
plt.imshow(pos_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Distinctive Positive Words (TF-IDF)")
plt.show()

# Plot negative
plt.figure(figsize=(10,5))
plt.imshow(neg_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Distinctive Negative Words (TF-IDF)")
plt.show()

# Plot Top N Positive/Negative Words Bar Chart
def plot_top_words(word_scores, title, color='green', top_n=20):
    words, scores = zip(*word_scores[:top_n])  # Unpack the top N words and scores
    plt.figure(figsize=(10, 6))
    plt.barh(words, scores, color=color)
    plt.xlabel('Average TF-IDF Score')
    plt.title(title)
    plt.gca().invert_yaxis()  # Highest score at the top
    plt.show()

# Plot top positive words
plot_top_words(pos_sorted, "Top 20 Positive Words", color='blue')

# Plot top negative words
plot_top_words(neg_sorted, "Top 20 Negative Words", color='green')

"""***Bonus*** Naive Bayes classifier"""

# Initialize and train the Naive Bayes classifier
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# Make predictions
y_pred_nb = nb_model.predict(X_test)

# Evaluate accuracy
nb_accuracy = accuracy_score(y_test, y_pred_nb)
print(f"\ Naive Bayes Accuracy: {nb_accuracy * 100:.2f}%\n")
print("Classification Report:\n", classification_report(y_test, y_pred_nb))

"""Comparison"""

# Generate confusion matrices
cm_lr = confusion_matrix(y_test, y_pred_lr)
cm_nb = confusion_matrix(y_test, y_pred_nb)

# Logistic Regression
plt.figure(figsize=(6, 4))
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")

# Naive Bayes
plt.figure(figsize=(6, 4))
sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Reds',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title("Confusion Matrix - Naive Bayes")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")

plt.show()


print(f"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr) * 100:.2f}%\n")
print(f"Naive Bayes Accuracy: {nb_accuracy * 100:.2f}%")

"""save models"""

# Save the TF-IDF Vectorizer
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

# Save Logistic Regression model
joblib.dump(lr_model, 'logistic_model.pkl')

# Save Naive Bayes model
joblib.dump(nb_model, 'naive_bayes_model.pkl')

print(" All models and vectorizer saved successfully!")